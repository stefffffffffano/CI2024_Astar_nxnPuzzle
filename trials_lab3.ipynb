{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Copyright **`(c)`** 2024 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free under certain conditions — see the [`license`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple,deque\n",
    "from heapq import heappush, heappop\n",
    "from random import choice\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUZZLE_DIM = 6\n",
    "action = namedtuple('Action', ['pos1', 'pos2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state: np.ndarray) -> list['Action']:\n",
    "    x, y = [int(_[0]) for _ in np.where(state == 0)]\n",
    "    actions = list()\n",
    "    if x > 0:\n",
    "        actions.append(action((x, y), (x - 1, y)))\n",
    "    if x < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x + 1, y)))\n",
    "    if y > 0:\n",
    "        actions.append(action((x, y), (x, y - 1)))\n",
    "    if y < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x, y + 1)))\n",
    "    return actions\n",
    "\n",
    "\n",
    "\n",
    "def do_action(state: np.ndarray, action: 'Action') -> np.ndarray:\n",
    "    new_state = state.copy()\n",
    "    new_state[action.pos1], new_state[action.pos2] = new_state[action.pos2], new_state[action.pos1]\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOMIZE_STEPS = 100_000\n",
    "state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "for r in tqdm(range(RANDOMIZE_STEPS), desc='Randomizing'):\n",
    "    state = do_action(state, choice(available_actions(state)))\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to solve the puzzle \n",
    "def is_solved(state: np.ndarray) -> bool:\n",
    "    return np.array_equal(state, np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM)))\n",
    "\n",
    "#Solve the puzzle with path search\n",
    "#We can introduce pruning: if the last move undoes the previous one, we can skip it \n",
    "def solve(state: np.ndarray) -> list['Action']:\n",
    "    last_action = None\n",
    "    visited = set()\n",
    "    queue = deque([(state, [])])  # Using deque for efficient FIFO operations\n",
    "    while queue:\n",
    "        current_state, current_path = queue.pop  () #BFS or DFS depending on popleft() or pop()\n",
    "        if is_solved(current_state):\n",
    "            return current_path\n",
    "        visited.add(state.tobytes()) #state.tobytes() for a more efficient hashable representation\n",
    "        last_action = current_state\n",
    "        for a in available_actions(current_state):\n",
    "            new_state = do_action(current_state, a)\n",
    "            if last_action is not None and np.array_equal(new_state, last_action):\n",
    "                continue\n",
    "            elif new_state.tobytes() not in visited:\n",
    "                queue.append((new_state, current_path + [a]))\n",
    "    return None\n",
    "\n",
    "\n",
    "solution = solve(state)\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_solved(state: np.ndarray) -> bool:\n",
    "    return np.array_equal(state, np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM)))\n",
    "\n",
    "def heuristic(state: np.ndarray) -> int:\n",
    "    target_positions = {val: (i, j) for i in range(PUZZLE_DIM) for j in range(PUZZLE_DIM) for val in [i * PUZZLE_DIM + j + 1]}\n",
    "    target_positions[0] = (PUZZLE_DIM - 1, PUZZLE_DIM - 1)  # Posizione target del blocco vuoto (0)\n",
    "\n",
    "    dist = 0\n",
    "    for i in range(PUZZLE_DIM):\n",
    "        for j in range(PUZZLE_DIM):\n",
    "            val = state[i, j]\n",
    "            if val != 0:  # Ignora il blocco vuoto\n",
    "                target_i, target_j = target_positions[val]\n",
    "                dist += abs(i - target_i) + abs(j - target_j)  # Distanza di Manhattan\n",
    "    return dist\n",
    "\n",
    "def solve2(state: np.ndarray) -> tuple[list, int]:\n",
    "    counter_action_evaluated = 0\n",
    "    visited = {}  # Stato in bytes -> costo minimo trovato\n",
    "    queue = []\n",
    "    heappush(queue, (0, state.tobytes(), []))  # Usa lo stato serializzato\n",
    "    visited[state.tobytes()] = 0  # Traccia lo stato iniziale con costo 0\n",
    "\n",
    "    while queue:\n",
    "        _, current_state_bytes, current_path = heappop(queue)\n",
    "        \n",
    "        # Decodifica lo stato da bytes a np.ndarray per verificare se è risolto\n",
    "        current_state = np.frombuffer(current_state_bytes, dtype=state.dtype).reshape(state.shape)\n",
    "        \n",
    "        # Verifica se il puzzle è risolto\n",
    "        if is_solved(current_state):\n",
    "            return (current_path,counter_action_evaluated)\n",
    "\n",
    "        # Genera nuovi stati per ogni azione possibile\n",
    "        for a in available_actions(current_state):\n",
    "            counter_action_evaluated += 1\n",
    "            new_state = do_action(current_state, a)\n",
    "            new_cost = len(current_path) + 1\n",
    "            new_state_bytes = new_state.tobytes()\n",
    "\n",
    "            # Se lo stato non è visitato o ha un costo inferiore\n",
    "            if new_state_bytes not in visited or visited[new_state_bytes] > new_cost:\n",
    "                visited[new_state_bytes] = new_cost\n",
    "                priority = new_cost + heuristic(new_state)\n",
    "\n",
    "                # Creiamo un nuovo percorso come copia di `current_path`\n",
    "                new_path = list(current_path) + [a]\n",
    "                \n",
    "                # Aggiungi alla coda di priorità usando `new_state_bytes`\n",
    "                heappush(queue, (priority, new_state_bytes, new_path))\n",
    "\n",
    "    return None\n",
    "\n",
    "solution,cost = solve2(state)\n",
    "#Joblib\n",
    "print(solution,cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_solved(state: np.ndarray) -> bool:\n",
    "    return np.array_equal(state, np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM)))\n",
    "#A* with improved heuristic\n",
    "def heuristic(state: np.ndarray) -> int:\n",
    "    PUZZLE_DIM = state.shape[0]\n",
    "    target_positions = {val: (i, j) for i in range(PUZZLE_DIM) for j in range(PUZZLE_DIM) for val in [i * PUZZLE_DIM + j + 1]}\n",
    "    target_positions[0] = (PUZZLE_DIM - 1, PUZZLE_DIM - 1)  # Posizione target del blocco vuoto (0)\n",
    "\n",
    "    manhattan_dist = 0\n",
    "    linear_conflict = 0\n",
    "\n",
    "    for i in range(PUZZLE_DIM):\n",
    "        for j in range(PUZZLE_DIM):\n",
    "            val = state[i, j]\n",
    "            if val != 0:\n",
    "                target_i, target_j = target_positions[val]\n",
    "                manhattan_dist += abs(i - target_i) + abs(j - target_j)\n",
    "\n",
    "                # Linear conflict: se due pezzi sono nella stessa riga o colonna\n",
    "                if i == target_i:  # Nella riga corretta\n",
    "                    for k in range(j + 1, PUZZLE_DIM):\n",
    "                        val2 = state[i, k]\n",
    "                        if val2 != 0:\n",
    "                            target_i2, target_j2 = target_positions[val2]\n",
    "                            if i == target_i2 and target_j2 < target_j:\n",
    "                                linear_conflict += 2\n",
    "                if j == target_j:  # Nella colonna corretta\n",
    "                    for k in range(i + 1, PUZZLE_DIM):\n",
    "                        val2 = state[k, j]\n",
    "                        if val2 != 0:\n",
    "                            target_i2, target_j2 = target_positions[val2]\n",
    "                            if j == target_j2 and target_i2 < target_i:\n",
    "                                linear_conflict += 2\n",
    "\n",
    "    return manhattan_dist + linear_conflict\n",
    "\n",
    "\n",
    "def solve3(state: np.ndarray) -> tuple[list, int]:\n",
    "    counter_action_evaluated = 0\n",
    "    visited = {}  # Stato in bytes -> costo minimo trovato\n",
    "    queue = []\n",
    "    heappush(queue, (0, state.tobytes(), []))  # Usa lo stato serializzato\n",
    "    visited[state.tobytes()] = 0  # Traccia lo stato iniziale con costo 0\n",
    "\n",
    "    while queue:\n",
    "        _, current_state_bytes, current_path = heappop(queue)\n",
    "        \n",
    "        # Decodifica lo stato da bytes a np.ndarray per verificare se è risolto\n",
    "        current_state = np.frombuffer(current_state_bytes, dtype=state.dtype).reshape(state.shape)\n",
    "        \n",
    "        # Verifica se il puzzle è risolto\n",
    "        if is_solved(current_state):\n",
    "            return (current_path,counter_action_evaluated)\n",
    "\n",
    "        # Genera nuovi stati per ogni azione possibile\n",
    "        for a in available_actions(current_state):\n",
    "            counter_action_evaluated += 1\n",
    "            new_state = do_action(current_state, a)\n",
    "            new_cost = len(current_path) + 1\n",
    "            new_state_bytes = new_state.tobytes()\n",
    "\n",
    "            # Se lo stato non è visitato o ha un costo inferiore\n",
    "            if new_state_bytes not in visited or visited[new_state_bytes] > new_cost:\n",
    "                visited[new_state_bytes] = new_cost\n",
    "                priority = new_cost + heuristic(new_state)\n",
    "\n",
    "                # Creiamo un nuovo percorso come copia di `current_path`\n",
    "                new_path = list(current_path) + [a]\n",
    "                \n",
    "                # Aggiungi alla coda di priorità usando `new_state_bytes`\n",
    "                heappush(queue, (priority, new_state_bytes, new_path))\n",
    "\n",
    "    return None\n",
    "\n",
    "solution,cost = solve3(state)\n",
    "#Joblib\n",
    "print(solution,cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhanced A* combining a lot of heuristics-> this is the one that works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PuzzleHeuristicService:\n",
    "    def __init__(self, goal_state: np.ndarray):\n",
    "        self.goal_state = goal_state\n",
    "\n",
    "    def heuristic_manhattan_distance(self, position: np.ndarray) -> int:\n",
    "        distance = 0\n",
    "        size = len(position)\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                tile = position[i][j]\n",
    "                if tile != 0:\n",
    "                    target_row = (tile - 1) // size\n",
    "                    target_col = (tile - 1) % size\n",
    "                    distance += abs(i - target_row) + abs(j - target_col)\n",
    "        return distance\n",
    "\n",
    "    def heuristic_linear_conflict(self, position: np.ndarray) -> int:\n",
    "        conflict = 0\n",
    "        size = len(position)\n",
    "\n",
    "        # Row conflicts\n",
    "        for row in range(size):\n",
    "            max_val = -1\n",
    "            for col in range(size):\n",
    "                value = position[row][col]\n",
    "                if value != 0 and (value - 1) // size == row:\n",
    "                    if value > max_val:\n",
    "                        max_val = value\n",
    "                    else:\n",
    "                        conflict += 2\n",
    "\n",
    "        # Column conflicts\n",
    "        for col in range(size):\n",
    "            max_val = -1\n",
    "            for row in range(size):\n",
    "                value = position[row][col]\n",
    "                if value != 0 and (value - 1) % size == col:\n",
    "                    if value > max_val:\n",
    "                        max_val = value\n",
    "                    else:\n",
    "                        conflict += 2\n",
    "\n",
    "        return conflict\n",
    "\n",
    "    def heuristic_walking_distance(self, position: np.ndarray) -> int:\n",
    "        # Calculate the Manhattan distance grid\n",
    "        size = len(position)\n",
    "        distance_grid = [[0] * size for _ in range(size)]\n",
    "\n",
    "        for row in range(size):\n",
    "            for col in range(size):\n",
    "                value = position[row][col]\n",
    "                if value != 0:\n",
    "                    target_row = (value - 1) // size\n",
    "                    target_col = (value - 1) % size\n",
    "                    distance_grid[row][col] = abs(row - target_row) + abs(col - target_col)\n",
    "\n",
    "        # Sum the distances\n",
    "        walking_distance = sum(sum(row) for row in distance_grid)\n",
    "        return walking_distance\n",
    "\n",
    "    def combined_heuristic(self, position: np.ndarray) -> int:\n",
    "        return (\n",
    "            self.heuristic_manhattan_distance(position)\n",
    "            + self.heuristic_linear_conflict(position)\n",
    "            + self.heuristic_walking_distance(position)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_with_enhanced_a_star(initial_state: np.ndarray, goal_state: np.ndarray) -> tuple[list, int]:\n",
    "    heuristic_service = PuzzleHeuristicService(goal_state)\n",
    "\n",
    "    def calculate_heuristic(state: np.ndarray) -> int:\n",
    "        return heuristic_service.combined_heuristic(state)\n",
    "\n",
    "    # Priority queue: (f_score, g_score, state_bytes, path)\n",
    "    open_set = []\n",
    "    heappush(open_set, (calculate_heuristic(initial_state), 0, initial_state.tobytes(), []))\n",
    "    visited = set()\n",
    "    goal_state_bytes = goal_state.tobytes()\n",
    "\n",
    "    counter_action_evaluated = 0\n",
    "\n",
    "    while open_set:\n",
    "        # Extract the node with the lowest f_score\n",
    "        f_score, g_score, current_bytes, path = heappop(open_set)\n",
    "        current_state = np.frombuffer(current_bytes, dtype=initial_state.dtype).reshape(initial_state.shape)\n",
    "\n",
    "        # Check if we've reached the goal state\n",
    "        if current_bytes == goal_state_bytes:\n",
    "            return path, counter_action_evaluated\n",
    "\n",
    "        # Add current state to visited\n",
    "        visited.add(current_bytes)\n",
    "\n",
    "        # Generate all possible moves\n",
    "        for act in available_actions(current_state):\n",
    "            counter_action_evaluated += 1\n",
    "            next_state = do_action(current_state, act)\n",
    "            next_bytes = next_state.tobytes()\n",
    "\n",
    "            if next_bytes in visited:\n",
    "                continue\n",
    "\n",
    "            # Update scores\n",
    "            new_g_score = g_score + 1\n",
    "            new_f_score = new_g_score + calculate_heuristic(next_state)\n",
    "\n",
    "            # Add new state to open set\n",
    "            heappush(open_set, (new_f_score, new_g_score, next_bytes, path + [act]))\n",
    "\n",
    "    return None, counter_action_evaluated  # No solution found\n",
    "\n",
    "goal_state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "path, evaluated_actions = solve_with_enhanced_a_star(state, goal_state)\n",
    "print(\"Path to solution:\", path)\n",
    "print(\"Number of actions evaluated:\", evaluated_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_solved(state: np.ndarray) -> bool:\n",
    "    return np.array_equal(state, np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM)))\n",
    "\n",
    "#Let's try to apply actions to the initial state to see if they work\n",
    "current_state = state.copy()\n",
    "for act in path:\n",
    "    current_state = do_action(current_state, act)\n",
    "print(\"Is the puzzle solved?\", is_solved(current_state))\n",
    "print(current_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union, List, Set\n",
    "\n",
    "class PuzzleHeuristicService:\n",
    "    def __init__(self, goal_state: np.ndarray):\n",
    "        self.goal_state = goal_state\n",
    "        size = goal_state.shape[0]\n",
    "        self.target_positions = {\n",
    "            tile: (tile // size, tile % size) for tile in range(1, size * size)\n",
    "        }\n",
    "        self.target_positions[0] = (size - 1, size - 1)\n",
    "\n",
    "    def heuristic_manhattan_distance(self, position: np.ndarray) -> int:\n",
    "        distance = 0\n",
    "        size = position.shape[0]\n",
    "        for row in range(size):\n",
    "            for col in range(size):\n",
    "                tile = position[row][col]\n",
    "                if tile != 0:\n",
    "                    target_row, target_col = self.target_positions[tile]\n",
    "                    distance += abs(row - target_row) + abs(col - target_col)\n",
    "        return distance\n",
    "\n",
    "    def heuristic_linear_conflict(self, position: np.ndarray) -> int:\n",
    "        conflict = 0\n",
    "        size = position.shape[0]\n",
    "\n",
    "        # Row conflicts\n",
    "        for row in range(size):\n",
    "            max_val = -1\n",
    "            for col in range(size):\n",
    "                value = position[row][col]\n",
    "                if value != 0 and (value - 1) // size == row:\n",
    "                    if value > max_val:\n",
    "                        max_val = value\n",
    "                    else:\n",
    "                        conflict += 2\n",
    "\n",
    "        # Column conflicts\n",
    "        for col in range(size):\n",
    "            max_val = -1\n",
    "            for row in range(size):\n",
    "                value = position[row][col]\n",
    "                if value != 0 and (value - 1) % size == col:\n",
    "                    if value > max_val:\n",
    "                        max_val = value\n",
    "                    else:\n",
    "                        conflict += 2\n",
    "\n",
    "        return conflict\n",
    "\n",
    "    def heuristic_walking_distance(self, position: np.ndarray) -> int:\n",
    "        size = position.shape[0]\n",
    "        distance = 0\n",
    "        for row in range(size):\n",
    "            for col in range(size):\n",
    "                tile = position[row][col]\n",
    "                if tile != 0:\n",
    "                    target_row, target_col = self.target_positions[tile]\n",
    "                    distance += abs(row - target_row) + abs(col - target_col)\n",
    "        return distance\n",
    "\n",
    "    def combined_heuristic(self, position: np.ndarray) -> int:\n",
    "        return (\n",
    "            self.heuristic_manhattan_distance(position)\n",
    "            + self.heuristic_linear_conflict(position)\n",
    "            + self.heuristic_walking_distance(position)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def ida_star(initial_state: np.ndarray, goal_state: np.ndarray) -> tuple[list, int]:\n",
    "    heuristic_service = PuzzleHeuristicService(goal_state)\n",
    "\n",
    "    def calculate_heuristic(state: np.ndarray) -> int:\n",
    "        return heuristic_service.combined_heuristic(state)\n",
    "\n",
    "    def search(state: np.ndarray, g: int, threshold: int, path: list, visited: set) -> tuple[int, Union[None, list]]:\n",
    "        \"\"\"\n",
    "        Ricerca limitata basata sul costo `f = g + h`.\n",
    "\n",
    "        Ritorna:\n",
    "        - Nuovo limite (`next_threshold`) se il nodo attuale supera il limite.\n",
    "        - Il percorso se lo stato obiettivo viene trovato.\n",
    "        \"\"\"\n",
    "        h = calculate_heuristic(state)\n",
    "        f = g + h\n",
    "\n",
    "        # Se il costo supera il limite, ritorna il nuovo limite\n",
    "        if f > threshold:\n",
    "            return f, None\n",
    "\n",
    "        # Se lo stato è quello obiettivo, ritorna il percorso\n",
    "        if np.array_equal(state, goal_state):\n",
    "            return f, path\n",
    "\n",
    "        # Aggiorna il limite successivo\n",
    "        next_threshold = float('inf')\n",
    "\n",
    "        # Genera mosse valide\n",
    "        for action in available_actions(state):\n",
    "            next_state = do_action(state, action)\n",
    "\n",
    "            # Evita cicli controllando gli stati già visitati\n",
    "            state_tuple = tuple(next_state.flatten())\n",
    "            if state_tuple in visited:\n",
    "                continue\n",
    "\n",
    "            visited.add(state_tuple)\n",
    "            new_g = g + 1\n",
    "            result, solution = search(next_state, new_g, threshold, path + [action], visited)\n",
    "            visited.remove(state_tuple)\n",
    "\n",
    "            # Se troviamo una soluzione, la ritorniamo immediatamente\n",
    "            if solution is not None:\n",
    "                return result, solution\n",
    "\n",
    "            # Aggiorna il prossimo limite\n",
    "            next_threshold = min(next_threshold, result)\n",
    "\n",
    "        return next_threshold, None\n",
    "\n",
    "    # Calcola il valore iniziale della soglia\n",
    "    threshold = calculate_heuristic(initial_state)\n",
    "    visited = {tuple(initial_state.flatten())}\n",
    "    path = []\n",
    "\n",
    "    while True:\n",
    "        # Esegui la ricerca limitata\n",
    "        next_threshold, solution = search(initial_state, 0, threshold, path, visited)\n",
    "\n",
    "        # Se troviamo una soluzione, la ritorniamo\n",
    "        if solution is not None:\n",
    "            return solution, len(solution)\n",
    "\n",
    "        # Se il limite non si aggiorna, significa che non ci sono soluzioni\n",
    "        if next_threshold == float('inf'):\n",
    "            return None, float('inf')\n",
    "\n",
    "        # Aggiorna la soglia\n",
    "        threshold = next_threshold\n",
    "\n",
    "goal_state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "path, evaluated_actions = ida_star(state, goal_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "import numpy as np\n",
    "\n",
    "class EnhancedPuzzleHeuristicService:\n",
    "    def __init__(self, goal_state: np.ndarray):\n",
    "        self.goal_state = goal_state\n",
    "\n",
    "    def heuristic_manhattan_distance(self, state: np.ndarray) -> int:\n",
    "        distance = 0\n",
    "        size = len(state)\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                tile = state[i][j]\n",
    "                if tile != 0:\n",
    "                    target_row = (tile - 1) // size\n",
    "                    target_col = (tile - 1) % size\n",
    "                    distance += abs(i - target_row) + abs(j - target_col)\n",
    "        return distance\n",
    "\n",
    "    def heuristic_linear_conflict(self, state: np.ndarray) -> int:\n",
    "        conflict = 0\n",
    "        size = state.shape[0]\n",
    "        for row in range(size):\n",
    "            row_goal = self.goal_state[row]\n",
    "            row_state = state[row]\n",
    "            for i in range(size):\n",
    "                for j in range(i + 1, size):\n",
    "                    if (\n",
    "                        row_state[i] in row_goal\n",
    "                        and row_state[j] in row_goal\n",
    "                        and row_state[i] > row_state[j]\n",
    "                    ):\n",
    "                        conflict += 2\n",
    "        return conflict\n",
    "\n",
    "    def combined_heuristic(self, state: np.ndarray) -> int:\n",
    "        # Usa Manhattan + Linear Conflict come combinazione euristica\n",
    "        return self.heuristic_manhattan_distance(state) + self.heuristic_linear_conflict(state)\n",
    "\n",
    "\n",
    "def enhanced_a_star(\n",
    "    initial_state: np.ndarray,\n",
    "    goal_state: np.ndarray,\n",
    "    heuristic_service: EnhancedPuzzleHeuristicService,\n",
    ") -> Tuple[Union[List, None], int]:\n",
    "    def calculate_heuristic(state: np.ndarray) -> int:\n",
    "        return heuristic_service.combined_heuristic(state)\n",
    "\n",
    "    open_set = []  # Priority queue: (f_score, state, path)\n",
    "    heappush(open_set, (calculate_heuristic(initial_state), 0, initial_state.tobytes(), []))\n",
    "    visited: Dict[bytes, int] = {}  # Stato serializzato -> costo minimo\n",
    "\n",
    "    evaluated_actions = 0\n",
    "    goal_bytes = goal_state.tobytes()\n",
    "\n",
    "    while open_set:\n",
    "        f_score, g_score, current_bytes, path = heappop(open_set)\n",
    "        current_state = np.frombuffer(current_bytes, dtype=initial_state.dtype).reshape(initial_state.shape)\n",
    "\n",
    "        # Se siamo arrivati al goal\n",
    "        if current_bytes == goal_bytes:\n",
    "            return path, evaluated_actions\n",
    "\n",
    "        # Se abbiamo già visitato questo nodo con un costo minore, lo saltiamo\n",
    "        if current_bytes in visited and visited[current_bytes] <= g_score:\n",
    "            continue\n",
    "\n",
    "        # Memorizza il costo migliore trovato finora\n",
    "        visited[current_bytes] = g_score\n",
    "\n",
    "        # Genera le azioni disponibili\n",
    "        for action in available_actions(current_state):\n",
    "            evaluated_actions += 1\n",
    "            next_state = do_action(current_state, action)\n",
    "            next_bytes = next_state.tobytes()\n",
    "            next_g_score = g_score + 1\n",
    "            next_f_score = next_g_score + calculate_heuristic(next_state)\n",
    "\n",
    "            if next_bytes not in visited or visited[next_bytes] > next_g_score:\n",
    "                heappush(open_set, (next_f_score, next_g_score, next_bytes, path + [action]))\n",
    "\n",
    "    return None, evaluated_actions  # Nessuna soluzione trovata\n",
    "\n",
    "goal_state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "heuristic_service = EnhancedPuzzleHeuristicService(goal_state)\n",
    "path, evaluated_actions = enhanced_a_star(state, goal_state, heuristic_service)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional A*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from heapq import heappush, heappop\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "\n",
    "def bidirectional_a_star(initial_state: np.ndarray, goal_state: np.ndarray) -> Tuple[List[int], int]:\n",
    "    def calculate_heuristic(state: np.ndarray) -> int:\n",
    "        return heuristic_service.combined_heuristic(state)\n",
    "    \n",
    "    # Priority queues for forward and backward searches\n",
    "    open_set_forward = []\n",
    "    open_set_backward = []\n",
    "    \n",
    "    # Visited states for both forward and backward searches\n",
    "    visited_forward: Dict[bytes, int] = {}\n",
    "    visited_backward: Dict[bytes, int] = {}\n",
    "    \n",
    "    # Initialize forward search\n",
    "    heappush(open_set_forward, (calculate_heuristic(initial_state), 0, initial_state.tobytes(), []))\n",
    "    visited_forward[initial_state.tobytes()] = 0\n",
    "    \n",
    "    # Initialize backward search\n",
    "    heappush(open_set_backward, (calculate_heuristic(goal_state), 0, goal_state.tobytes(), []))\n",
    "    visited_backward[goal_state.tobytes()] = 0\n",
    "    \n",
    "    # The optimal state in bytes\n",
    "    initial_bytes = initial_state.tobytes()\n",
    "    goal_bytes = goal_state.tobytes()\n",
    "    \n",
    "    evaluated_actions = 0\n",
    "    meet_node = None  # This will store the meeting point\n",
    "    \n",
    "    while open_set_forward and open_set_backward:\n",
    "        # Forward search\n",
    "        f_score_f, g_score_f, current_bytes_f, path_f = heappop(open_set_forward)\n",
    "        current_state_f = np.frombuffer(current_bytes_f, dtype=initial_state.dtype).reshape(initial_state.shape)\n",
    "\n",
    "        # If meeting point found from forward search\n",
    "        if current_bytes_f in visited_backward:\n",
    "            meet_node = current_bytes_f\n",
    "            total_cost = g_score_f + visited_backward[meet_node]\n",
    "            # Reconstruct and return the combined path\n",
    "            combined_path = path_f + path_b[::-1]  # Reverse the backward path\n",
    "            return combined_path, evaluated_actions\n",
    "        \n",
    "        # Generate actions for forward search\n",
    "        for action in available_actions(current_state_f):\n",
    "            evaluated_actions += 1\n",
    "            next_state_f = do_action(current_state_f, action)\n",
    "            next_bytes_f = next_state_f.tobytes()\n",
    "            next_g_score_f = g_score_f + 1\n",
    "            next_f_score_f = next_g_score_f + calculate_heuristic(next_state_f)\n",
    "            \n",
    "            if next_bytes_f not in visited_forward or visited_forward[next_bytes_f] > next_g_score_f:\n",
    "                visited_forward[next_bytes_f] = next_g_score_f\n",
    "                heappush(open_set_forward, (next_f_score_f, next_g_score_f, next_bytes_f, path_f + [action]))\n",
    "\n",
    "        # Backward search\n",
    "        f_score_b, g_score_b, current_bytes_b, path_b = heappop(open_set_backward)\n",
    "        current_state_b = np.frombuffer(current_bytes_b, dtype=goal_state.dtype).reshape(goal_state.shape)\n",
    "\n",
    "        # If meeting point found from backward search\n",
    "        if current_bytes_b in visited_forward:\n",
    "            meet_node = current_bytes_b\n",
    "            total_cost = g_score_b + visited_forward[meet_node]\n",
    "            # Reconstruct and return the combined path\n",
    "            combined_path = path_b + path_f[::-1]  # Reverse the forward path\n",
    "            return combined_path, evaluated_actions\n",
    "        \n",
    "        # Generate actions for backward search\n",
    "        for action in available_actions(current_state_b):\n",
    "            evaluated_actions += 1\n",
    "            next_state_b = do_action(current_state_b, action)\n",
    "            next_bytes_b = next_state_b.tobytes()\n",
    "            next_g_score_b = g_score_b + 1\n",
    "            next_f_score_b = next_g_score_b + calculate_heuristic(next_state_b)\n",
    "            \n",
    "            if next_bytes_b not in visited_backward or visited_backward[next_bytes_b] > next_g_score_b:\n",
    "                visited_backward[next_bytes_b] = next_g_score_b\n",
    "                heappush(open_set_backward, (next_f_score_b, next_g_score_b, next_bytes_b, path_b + [action]))\n",
    "\n",
    "    return None, evaluated_actions  # No solution found\n",
    "goal_state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "path, evaluated_actions = bidirectional_a_star(state, goal_state)\n",
    "print(\"Path to solution:\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the best version of previously defined A* and trying to parallelize the job with Joblib to speedup computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop\n",
    "from typing import List, Tuple, Set\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "\n",
    "def solve_with_enhanced_a_star_parallel(initial_state: np.ndarray, goal_state: np.ndarray, n_jobs: int = -1) -> Tuple[List[int], int]:\n",
    "    heuristic_service = PuzzleHeuristicService(goal_state)\n",
    "\n",
    "    def calculate_heuristic(state: np.ndarray) -> int:\n",
    "        return heuristic_service.combined_heuristic(state)\n",
    "\n",
    "    # Priority queue: (f_score, g_score, state_bytes, path)\n",
    "    open_set = []\n",
    "    heappush(open_set, (calculate_heuristic(initial_state), 0, initial_state.tobytes(), []))\n",
    "    visited: Set[bytes] = set()\n",
    "    goal_state_bytes = goal_state.tobytes()\n",
    "\n",
    "    counter_action_evaluated = 0\n",
    "\n",
    "    # Parallelize the generation of next states using joblib\n",
    "    def generate_next_states(state: np.ndarray, g_score: int, path: List[int]) -> List[Tuple[int, int, np.ndarray, List[int]]]:\n",
    "        nonlocal counter_action_evaluated\n",
    "        next_states = []\n",
    "        for act in available_actions(state):\n",
    "            counter_action_evaluated += 1\n",
    "            next_state = do_action(state, act)\n",
    "            next_bytes = next_state.tobytes()\n",
    "\n",
    "            if next_bytes in visited:\n",
    "                continue\n",
    "\n",
    "            # Update scores\n",
    "            new_g_score = g_score + 1\n",
    "            new_f_score = new_g_score + calculate_heuristic(next_state)\n",
    "\n",
    "            next_states.append((new_f_score, new_g_score, next_state, path + [act]))\n",
    "\n",
    "        return next_states\n",
    "\n",
    "    while open_set:\n",
    "        # Extract the node with the lowest f_score\n",
    "        f_score, g_score, current_bytes, path = heappop(open_set)\n",
    "        current_state = np.frombuffer(current_bytes, dtype=initial_state.dtype).reshape(initial_state.shape)\n",
    "\n",
    "        # Check if we've reached the goal state\n",
    "        if current_bytes == goal_state_bytes:\n",
    "            return path, counter_action_evaluated\n",
    "\n",
    "        # Add current state to visited\n",
    "        visited.add(current_bytes)\n",
    "\n",
    "        # Parallelize the generation of the next possible states\n",
    "        next_states_list = Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
    "            delayed(generate_next_states)(current_state, g_score, path)\n",
    "            for _ in range(n_jobs)\n",
    "        )\n",
    "\n",
    "        # Flatten the list of next states\n",
    "        next_states = [state for sublist in next_states_list for state in sublist]\n",
    "\n",
    "        # Add all new states to open set\n",
    "        for next_f_score, next_g_score, next_state, next_path in next_states:\n",
    "            next_bytes = next_state.tobytes()\n",
    "\n",
    "            # Only consider states with a better g_score\n",
    "            if next_bytes not in visited:\n",
    "                heappush(open_set, (next_f_score, next_g_score, next_bytes, next_path))\n",
    "\n",
    "    return None, counter_action_evaluated  # No solution found\n",
    "\n",
    "\n",
    "# Test the parallelized A* search\n",
    "goal_state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "path, evaluated_actions = solve_with_enhanced_a_star_parallel(state, goal_state, n_jobs=20)\n",
    "\n",
    "print(\"Path to solution:\", path)\n",
    "print(\"Number of actions evaluated:\", evaluated_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for act in path:\n",
    "    current_state = do_action(current_state, act)\n",
    "print(\"Is the puzzle solved?\", is_solved(current_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
